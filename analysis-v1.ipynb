{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSUEDO STEPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Import/Collection and Preparation\n",
    "    - Load and review the obtained APS Failure at Scania Trucks dataset from the UCI Machine Learning Repository.\n",
    "    - Perform initial data exploration:\n",
    "        - Analyze feature distributions\n",
    "        - Check for missing values\n",
    "        - Identify potential outliers\n",
    "    - Handle missing values as described in your methodology:\n",
    "        - Discard features with >70% missing values\n",
    "        - Drop rows for features with <5% missing values\n",
    "        - Apply median imputation for features with 5-15% missing values\n",
    "        - Use Multiple Imputation by Chained Equations (MICE) for features with 15-70% missing values\n",
    "    - Conduct feature engineering:\n",
    "        - Create binary features to indicate originally missing values\n",
    "        - Separate histogram bin features from numerical features\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "    - Visualize feature distributions and correlations\n",
    "    - Analyze class imbalance (59,000 negative vs. 1,000 positive cases)\n",
    "    - Identify potential key indicators of APS failures through statistical analysis:\n",
    "        - Correlation analysis\n",
    "        - Univariate studies\n",
    "        - Feature importance rankings using techniques like Recursive Feature Elimination (RFE) or Sklearn.SelectFromModel\n",
    "3. Model Development and Optimization\n",
    "    - Split the data into training (60,000 samples) and testing (16,000 samples) sets\n",
    "    - Implement k-fold cross-validation on the training set\n",
    "    - Train and evaluate multiple machine learning models:\n",
    "        - Logistic Regression (baseline)\n",
    "        - Random Forest\n",
    "        - Support Vector Machines (SVM)\n",
    "        - Gradient Boosted Decision Trees\n",
    "        - Neural Networks (sklearn.MLPClassifier)\n",
    "    - Optimize hyperparameters for each model using techniques like grid search or random search\n",
    "    - Address class imbalance using methods such as:\n",
    "        - Oversampling (e.g., SMOTE)\n",
    "        - Undersampling\n",
    "        - Class weighting\n",
    "4. Model Evaluation and Comparison\n",
    "    - Evaluate models using performance metrics:\n",
    "        - Accuracy\n",
    "        - Precision\n",
    "        - Recall\n",
    "        - Macro-F1 Score\n",
    "    - Compare model performance and select the best-performing model(s)\n",
    "    - Analyze feature importance for the selected model(s) to identify key indicators of APS failures\n",
    "5. Model Interpretation and Insights\n",
    "    - Interpret the results of the best-performing model(s)\n",
    "    - Identify the most significant features contributing to APS failures\n",
    "    - Develop actionable insights for improving maintenance strategies\n",
    "6. Cost-Benefit Analysis\n",
    "    - Estimate the potential cost savings from implementing the predictive maintenance model\n",
    "    - Compare the costs of false positives (unnecessary checks) vs. false negatives (missed failures)\n",
    "    - Analyze the impact of the predictive model on overall maintenance costs\n",
    "7. Model Deployment and Validation\n",
    "    - Implement the selected model(s) on the test set\n",
    "    - Evaluate the model's performance on unseen data\n",
    "    - Develop a strategy for model deployment in real-world scenarios\n",
    "    - Create a plan for continuous model monitoring and updating\n",
    "8. Documentation and Reporting\n",
    "    - Document the entire process, including data preprocessing steps, model development, and evaluation results\n",
    "    - Prepare visualizations and summary statistics to support findings\n",
    "    - Write a comprehensive report addressing the research questions and objectives\n",
    "    - Develop recommendations for implementing predictive maintenance strategies in Scania trucks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Import and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load and review the obtained APS Failure at Scania Trucks dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initial Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Analyze features distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Check for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key observations:\n",
    "\n",
    "1. High Missing Value Percentages (>70%): Several features (br_000, bq_000, bp_000, bo_000, ab_000, cr_000, bn_000) have extremely high missing value rates, ranging from 73.3% to 82.1%\n",
    "The highest missing rate is 82.1% for feature br_000\n",
    "\n",
    "\n",
    "2. Medium Missing Value Percentages (30-70%): Features bm_000, bl_000, and bk_000 show moderate to high missing values\n",
    "    - bm_000 has about 65.9% missing values\n",
    "    - bl_000 has 45.5% missing values\n",
    "    - bk_000 has 38.4% missing values\n",
    "\n",
    "\n",
    "3. Low Missing Value Percentages (~24-23%): A large cluster of features (ad_000, cg_000, ch_000, cf_000, co_000, cx_000, cz_000, cy_000, dc_000, db_000) have consistent missing value rates around 23-24.8%\n",
    "\n",
    "**This pattern suggests:**\n",
    "\n",
    "* The data collection process might have systematic issues for certain sensors/measurements\n",
    "* Features with very high missing values (>70%) might need to be dropped or require advanced imputation techniques\n",
    "* The consistent ~24% missing rate across multiple features might indicate a systematic data collection issue or a specific operational condition where these measurements weren't recorded\n",
    "* For machine learning purposes, handling these missing values will be crucial as they could significantly impact model performance\n",
    "\n",
    "For this APS failure data, these missing values might represent sensor failures or conditions where measurements couldn't be taken during vehicle operation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Handle Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Remove features with > 70% missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Drop rows for features with <5% missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Apply median imputation for features with 5-15% missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.4 Use Multiple Imputation by Chained Equations (MICE) for features with 15-70% missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Perform Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Separate histogram bin features from numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Create binary features to indicate originally missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 Split Training Dataset to training and target sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 Separate histogram bin features from numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Visualize feature distributions and correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Analyze class imbalance (59,000 negative vs. 1,000 positive cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Identify potential key indicators of APS failures through statistical analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Development and Optimization\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Split the data into training (60,000 samples) and testing (16,000 samples) sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Implement k-fold cross-validation on the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train and evaluate multiple machine learning models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Logistic Regression (baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4 Gradient Boosted Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.5 Neural Networks (sklearn.MLPClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Optimize hyperparameters for each model using techniques like grid search or random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Address class imbalance using methods such as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1 Oversampling (e.g., SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2 Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.3 Class weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Evaluate models using performance metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4 Macro-F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.5 Compare model performance and select the best-performing model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Interpretation and Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1 Interpret the results of the best-performing model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5.2 Identify the most significant features contributing to APS failures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Develop actionable insights for improving maintenance strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4     - Develop actionable insights for improving maintenance strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cost-Benefit Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Estimate the potential cost savings from implementing the predictive maintenance model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Compare the costs of false positives (unnecessary checks) vs. false negatives (missed failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Analyze the impact of the predictive model on overall maintenance costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Deployment and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Implement the selected model(s) on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Evaluate the model's performance on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Develop a strategy for model deployment in real-world scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. Create a plan for continuous model monitoring and updating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Documentation and Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. Document the entire process, including data preprocessing steps, model development, and evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Prepare visualizations and summary statistics to support findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3. Write a comprehensive report addressing the research questions and objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4. Develop recommendations for implementing predictive maintenance strategies in Scania trucks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Analyze feature importance for the selected model(s) to identify key indicators of APS failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42),\n",
    "    'LightGBM': LGBMClassifier(random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "    'BaggingClassifier': BaggingClassifier(random_state=42),\n",
    "}\n",
    "\n",
    "estimators = [(name, model) for name, model in models.items()]\n",
    "\n",
    "models['VotingClassifier'] = VotingClassifier(estimators=estimators, voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "print(\"\\nTraining and evaluating models...\")\n",
    "metrics = ['accuracy_score', \n",
    "           'precision_score', \n",
    "           'recall_score',\n",
    "           'f1_score']\n",
    "\n",
    "accuracy_df = pd.DataFrame(index = metrics)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # Convert to dataframe\n",
    "    accuracy_df[f'{name}'] = pd.Series([accuracy, precision, recall, f1])\n",
    "    \n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPerforming hyperparameter tuning...\")\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'Logistic Regression': {'C': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "    'Random Forest': {'n_estimators': [100, 200, 300], 'max_depth': [None, 10, 20, 30]},\n",
    "    'Decision Tree': {'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]},\n",
    "    'SVM': {'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear']},\n",
    "    'GradientBoosting': {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.1, 0.2]},\n",
    "    'XGBoost': {'n_estimators': [100, 200], 'max_depth': [3, 5, 7], 'learning_rate': [0.01, 0.1]},\n",
    "    'LightGBM': {'n_estimators': [100, 200], 'max_depth': [-1, 5, 10], 'learning_rate': [0.01, 0.1]},\n",
    "    'AdaBoost': {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1, 1]},\n",
    "    'BaggingClassifier': {'n_estimators': [10, 20, 30], 'max_samples': [0.5, 0.7, 1.0]},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.7' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/EMSOFT Analytics/AppData/Local/Microsoft/WindowsApps/python3.12.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Perform GridSearchCV for each model\n",
    "best_models = {}\n",
    "for name, model in models.items():\n",
    "    if name != 'VotingClassifier':  # Skip VotingClassifier for individual tuning\n",
    "        print(f\"\\nTuning {name}...\")\n",
    "        grid_search = GridSearchCV(model, param_grids[name], cv=3, scoring='f1_macro', n_jobs=-1)\n",
    "        grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "        \n",
    "        best_models[name] = grid_search.best_estimator_\n",
    "        \n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Best Macro-F1 Score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new VotingClassifier with the best models\n",
    "best_estimators = [(name, model) for name, model in best_models.items()]\n",
    "best_voting_classifier = VotingClassifier(estimators=best_estimators, voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the best VotingClassifier\n",
    "print(\"\\nTraining and evaluating the best VotingClassifier...\")\n",
    "best_voting_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = best_voting_classifier.predict(X_test)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Best VotingClassifier Macro-F1 Score: {macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAnalyzing feature importance...\")\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "rfc.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rfc.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
    "plt.title('Top 20 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nAnalysis complete. Check the generated plots for visualizations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "Shetty, R. (2021). Predicting a Failure in Scania’s Air Pressure System. [online] Medium. Available at: https://towardsdatascience.com/predicting-a-failure-in-scanias-air-pressure-system-aps-c260bcc4d038 [Accessed 4 Jan. 2025]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
